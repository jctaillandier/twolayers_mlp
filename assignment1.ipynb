{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.loadtxt(open(\"mnist_train.csv\", \"rb\"), delimiter=\",\", skiprows=1)\n",
    "\n",
    "train_labels = x[:50000, 0]\n",
    "train_data = x[:50000, 1:]\n",
    "\n",
    "valid_labels = x[50000:57000, 0]\n",
    "valid_data = x[50000:57000, 1:]\n",
    "\n",
    "test_labels = x[57000:,0]\n",
    "test_data = x[57000:,1:]\n",
    "\n",
    "unseen_data = np.loadtxt(open(\"mnist_test.csv\", \"rb\"), delimiter=\",\", skiprows=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    \n",
    "    return x*np.int64(x>0)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(X):\n",
    "    exps = np.exp(X - np.max(X))\n",
    "    return np.array(exps / np.sum(exps))\n",
    "\n",
    "\n",
    "def non_linearity(x, nonlin='relu'):\n",
    "    if nonlin == 'relu':\n",
    "        return x*np.int64(x>0)\n",
    "    elif nonlin == 'sigmoid':\n",
    "        return 1 / (1 + np.exp(-1*x))\n",
    "    elif nonlin == 'softmax':\n",
    "        exps = np.exp(x - np.max(x))\n",
    "        return exps / np.sum(exps)\n",
    "        \n",
    "#//////////////////////////////////////////////////////////////////\n",
    "def rand_index(upper_bound):\n",
    "    return np.random.randint(0, upper_bound)\n",
    "\n",
    "\n",
    "\n",
    "#////////////////////////////////////////////////////////////////////\n",
    "\n",
    "def cross_entr_loss(prediction, true_label):\n",
    "    # Calculate cross entropy for each output vector element , then sum up\n",
    "    loss_sum = 0 \n",
    "    y = np.eye(10)[int(true_label)]\n",
    "    \n",
    "    for j in range(10):\n",
    "        loss= np.zeros(10)\n",
    "        if y[j] == 1:\n",
    "            loss[j] = -np.log(prediction[j])\n",
    "        else:\n",
    "            loss[j] = -np.log(1 - prediction[j])\n",
    "            \n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three different ways to initialize parameters\n",
    "\n",
    "def initialize_weights( in_dim, out_dim, technique='normal', sigma=1.2, mu=0):\n",
    "        # Init with zeros for now\n",
    "        if technique =='zeros':\n",
    "            return np.zeros((in_dim,out_dim))\n",
    "        elif technique == 'normal':\n",
    "            return sigma * np.random.randn(in_dim, out_dim) + mu\n",
    "        #elif technique == 'glorot':\n",
    "            #return np.rand ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets of weights are: w_1 and b1 for layer 1, up to layer 3\n",
    "# pre actiavation neuron outputs are referred to as h1_a for layer 1\n",
    "# post activation neuron output are h1_o for layer 1\n",
    "# Note that I note h3 instead of a more common ho for the output layer\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    \n",
    "    def __init__(self, hidden_dims=(500,500),n_hidden=2, step_size = 0.0001, train_data=train_data,\n",
    "                 train_labels=train_labels, valid_data=valid_data, valid_labels=valid_labels, classes=10):\n",
    "        \n",
    "        \n",
    "        self.train_data = train_data\n",
    "        self.train_labels = train_labels\n",
    "        self.valid_data = valid_data\n",
    "        self.valid_labels = valid_labels\n",
    "        \n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.input_dim = train_data.shape[1]\n",
    "        self.classes = classes\n",
    "        \n",
    "        self.w_1 = initialize_weights(hidden_dims[0], self.input_dim)\n",
    "        self.w_2 = initialize_weights(hidden_dims[0],hidden_dims[1])        \n",
    "        self.w_3 = initialize_weights(classes, hidden_dims[1])\n",
    "        \n",
    "        self.b1 = np.zeros(hidden_dims[0])\n",
    "        self.b2 = np.zeros(hidden_dims[1])\n",
    "        self.b3 = np.zeros(classes)\n",
    "        \n",
    " \n",
    "            \n",
    "    def forward_prop(self, x, non_lin='relu'):\n",
    "        # Simply passes forward on input vector\n",
    "        # return output pre and post activation at each layer\n",
    "        h1_a = np.matmul(self.w_1, x) + self.b1 \n",
    "        #print('h1_a: ', h1_a.shape)\n",
    "\n",
    "        h1_o = non_linearity(h1_a, non_lin)\n",
    "        #print('h1_0: ', h1_o.shape)\n",
    "        h2_a = np.matmul(self.w_2 , h1_o) + self.b2 \n",
    "        #print('h2_a: ', h2_a.shape)\n",
    "        h2_o = non_linearity(h2_a, non_lin)\n",
    "        #print('h2_0: ', h2_o.shape)\n",
    "        h3_a = np.matmul(self.w_3 , h2_o) + self.b3 \n",
    "        h3_o = non_linearity(h3_a, nonlin='softmax')\n",
    "        \n",
    "        return (h1_a, h1_o, h2_a, h2_o, h3_a, h3_o)  \n",
    "    \n",
    "    def backward_prop(self, h1_a, h1_o, h2_a, h2_o, h3_a, h3_o, inputs, labels):\n",
    "        \n",
    "        # Find gradient of loss with respect to each parameters\n",
    "        \n",
    "        stuff = np.eye(self.classes)[int(labels)].reshape(self.classes,)\n",
    "        grad_h3a = h3_o - stuff\n",
    "        grad_b3 = grad_h3a\n",
    "        grad_W3 = np.matmul(grad_h3a, np.transpose(h3_o))\n",
    "        grad_h2o = np.matmul(np.transpose(self.w_3), grad_h3a)\n",
    "        grad_h2a = grad_h2o * np.int64(h2_a>0)\n",
    "        grad_b2 = grad_h2a\n",
    "        grad_W2 = np.matmul(grad_h2a, np.transpose(h1_o))\n",
    "        \n",
    "        grad_h1o = np.matmul(np.transpose(self.w_2), grad_h2a)\n",
    "        grad_h1a = grad_h1o * np.int64(h1_a > 0)\n",
    "        grad_b1 = grad_h1a\n",
    "        grad_W1 = np.matmul(grad_h1a.reshape(500,1), np.transpose(inputs.reshape(784,1)))\n",
    "                \n",
    "        return (grad_W1, grad_b1, grad_W2, grad_b2, grad_W3, grad_b3)\n",
    "    \n",
    "    # A function that trains a whole minibatch and average the gradients, to be sent for parameter update\n",
    "    def train_minibatch(self, batch_size):\n",
    "        # fprop and bprop for batch_size inputs, each time summing grads\n",
    "        \n",
    "        sum_grad_W1 = np.zeros((self.w_1.shape[0], self.w_1.shape[1]))\n",
    "        sum_grad_W2 = np.zeros((self.w_2.shape[0], self.w_2.shape[1]))\n",
    "        sum_grad_W3 = np.zeros((self.w_3.shape[0], self.w_3.shape[1]))\n",
    "        sum_grad_b1 = np.zeros(self.b1.shape[0])\n",
    "        sum_grad_b2 = np.zeros(self.b2.shape[0])\n",
    "        sum_grad_b3 = np.zeros(self.b3.shape[0])\n",
    "\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            z = rand_index(self.train_data.shape[0])\n",
    "            (h1_a, h1_o, h2_a, h2_o, h3_a, h3_o)  = self.forward_prop(train_data[z,:])\n",
    "            (grad_W1, grad_b1, grad_W2, grad_b2, grad_W3, grad_b3) = self.backward_prop(h1_a, h1_o, h2_a, h2_o, h3_a, h3_o, train_data[z,:] , train_labels[z] )\n",
    "            sum_grad_W1 = np.add(sum_grad_W1, grad_W1)\n",
    "            sum_grad_b1 = np.add(sum_grad_b1, grad_b1)\n",
    "            sum_grad_W2 = np.add(sum_grad_W2, grad_W2)\n",
    "            sum_grad_b2 = np.add(sum_grad_b2, grad_b2)\n",
    "            sum_grad_W3 = np.add(sum_grad_W3, grad_W3)\n",
    "            sum_grad_b3 = np.add(sum_grad_b3, grad_b3)\n",
    "        # return average of each gradients over the batch\n",
    "        return (sum_grad_W1/batch_size, sum_grad_b1/batch_size, sum_grad_W2/batch_size, sum_grad_b2/batch_size, \n",
    "                sum_grad_W3/batch_size, sum_grad_b3/batch_size)\n",
    "            \n",
    "    \n",
    "    \n",
    "    #Now that we have gradients, build a function to update all values using those gradient\n",
    "    def update_params(self, grad_W1, grad_b1, grad_W2, grad_b2, grad_W3, grad_b3, step_size):\n",
    "        self.w_1 = self.w_1 - (step_size * grad_W1 )\n",
    "        self.b1 = self.b1 - (step_size * grad_b1)\n",
    "        self.w_2 = self.w_2 - (step_size * grad_W2)\n",
    "        self.b2 = self.b2 - (step_size * grad_b2)\n",
    "        self.w_3 = self.w_3 - (step_size * grad_W3)\n",
    "        self.b3 = self.b3 - (step_size * grad_b3)\n",
    "        \n",
    "    # Full training, for n epochs\n",
    "    def train(self, batch_size, epochs, step_size):\n",
    "        for j in range(epochs):\n",
    "            sum_loss = 0\n",
    "            (grad_w1, grad_b1, grad_w2, grad_b2, grad_w3, grad_b3 ) = self.train_minibatch(batch_size)\n",
    "            self.update_params(grad_w1, grad_b1, grad_w2, grad_b2, grad_w3, grad_b3, step_size)\n",
    "            # after updating, calculate and output average loss\n",
    "            sum_loss = self.average_train_loss()\n",
    "            print('Average loss on training set after ',j+1 ,' epochs: ', sum_loss*100, '%')\n",
    "\n",
    "        #then calculate loss on validation set\n",
    "        valid_loss = self.average_valid_loss()\n",
    "        print('Loss on validation set after ', (epochs+1),' epochs of training (with step size = ', step_size,'): ', (valid_loss*100), '%')\n",
    "        \n",
    "        \n",
    "    # prediction function for 1 input, with current mlp parameters values\n",
    "    def predict(self, inputs):\n",
    "        x = np.matmul(self.w_1, inputs) + self.b1\n",
    "        x = relu(x)\n",
    "        x = np.matmul(self.w_2, x) + self.b2\n",
    "        x = relu(x)\n",
    "        x = np.matmul(self.w_3, x) + self.b3\n",
    "        output = softmax(x)\n",
    "                \n",
    "        return argmax(output)\n",
    "    \n",
    "    # calculate average loss over whole train set\n",
    "    def average_train_loss(self):\n",
    "        loss = 0\n",
    "        randindex = rand_index(self.train_data.shape[0])\n",
    "        # summing loss\n",
    "        for k in range(self.train_data.shape[0]):\n",
    "            (h1_a, h1_o, h2_a, h2_o, h3_a, h3_o) = self.forward_prop(train_data[randindex,...])    \n",
    "            one_loss = np.sum(cross_entr_loss(h3_o, train_labels[randindex]))\n",
    "            loss += one_loss\n",
    "            \n",
    "        return loss/self.train_data.shape[0]\n",
    "        \n",
    "    def average_valid_loss(self):\n",
    "        loss = 0\n",
    "        randindex = rand_index(self.valid_data.shape[0])\n",
    "        # summing loss\n",
    "        for k in range(self.valid_data.shape[0]):\n",
    "            (h1_a, h1_o, h2_a, h2_o, h3_a, h3_o) = self.forward_prop(valid_data[randindex,...])    \n",
    "            one_loss = np.sum(cross_entr_loss(h3_o, valid_labels[randindex]))\n",
    "            loss += one_loss\n",
    "        return (loss/self.train_data.shape[0])\n",
    "    \n",
    "    def test_misclass(self):\n",
    "        misclass = 0\n",
    "        \n",
    "        for k in range(test_data.shape[0]):\n",
    "            (h1_a, h1_o, h2_a, h2_o, h3_a, h3_o) = self.forward_prop(test_data[k,:])\n",
    "            pred = np.argmax(h3_o)\n",
    "            if(pred != int(test_labels[k])):\n",
    "                misclass += 1\n",
    "        \n",
    "        print('Misclassification rate on test set: ' , (misclass/test_data.shape[0])*100, '%')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.MLP object at 0x000002A9E5659F28>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:38: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\Public\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:40: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLP()\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:40: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\Public\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:38: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "# pred output --> (h1_a, h1_o, h2_a, h2_o, h3_a, h3_o) \n",
    "pred = mlp.forward_prop(train_data[1,...])\n",
    "output_vect = pred[5]\n",
    "print(output_vect)\n",
    "loss_sum = cross_entr_loss(output_vect, int(train_labels[4]) )\n",
    "print(np.sum(loss_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#self, h1_a, h1_o, h2_a, h2_o, h3_a, h3_o, inputs, labels\n",
    "#(grad_W1, grad_b1, grad_W2, grad_b2, grad_W3, grad_b3) = mlp.backward_prop(pred[0], pred[1], pred[2], pred[3], pred[4], pred[5], train_data[1,...], np.argmax(train_labels[4]))\n",
    "\n",
    "#print(grad_W1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on training set after  1  epochs:  10.536919344641047 %\n",
      "Average loss on training set after  2  epochs:  10.536849766957005 %\n",
      "Average loss on training set after  3  epochs:  10.537127453151308 %\n",
      "Average loss on training set after  4  epochs:  10.537578801094895 %\n",
      "Average loss on training set after  5  epochs:  10.538203785008958 %\n",
      "Average loss on training set after  6  epochs:  10.538481374652706 %\n",
      "Average loss on training set after  7  epochs:  10.538238018096822 %\n",
      "Average loss on training set after  8  epochs:  10.538689284056334 %\n",
      "Average loss on training set after  9  epochs:  10.538445880810164 %\n",
      "Average loss on training set after  10  epochs:  230.23602820064264 %\n",
      "Loss on validation set after  11  epochs of training (with step size =  0.001 ):  32.23304394811892 %\n"
     ]
    }
   ],
   "source": [
    "#def train(self, batch_size, epochs, step_size):\n",
    "mlp.train(64, 10, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on training set after  1  epochs:  10.53761391009291 %\n",
      "Average loss on training set after  2  epochs:  10.537023395835208 %\n",
      "Average loss on training set after  3  epochs:  10.53712744169346 %\n",
      "Average loss on training set after  4  epochs:  10.537578744360204 %\n",
      "Average loss on training set after  5  epochs:  230.24226737760998 %\n",
      "Loss on validation set after  6  epochs of training (with step size =  0.001 ):  1.4752998941267002 %\n"
     ]
    }
   ],
   "source": [
    "#def train(self, batch_size, epochs, step_size):\n",
    "mlp.train(64, 5, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassification rate on test set:  0.8886666666666667\n"
     ]
    }
   ],
   "source": [
    "# calculate 1-0 loss on test seen\n",
    "mlp.test_misclass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was taken from my Homework 3 of Ioannis' Machine Learning class in Automn '18'\n",
    "def finite_diff(self, inputs, labels):\n",
    "        #for each parameters we vary by epsilon one of its value and compute the finite gradient\n",
    "        #(f(x-eps) - f(x+eps))/eps\n",
    "        epsilon = 10**(-4)\n",
    "        grads_finite = []\n",
    "        for i,mat in enumerate([self.W1, self.b1, self.W2, self.b2]):\n",
    "            temp_mat_grad = np.zeros((len(mat), len(mat[0])))\n",
    "            for row in range(len(mat)):\n",
    "                for col in range(len(mat[0])):\n",
    "                    W1 = np.copy(self.W1)\n",
    "                    b1 = np.copy(self.b1)\n",
    "                    W2 = np.copy(self.W2)\n",
    "                    b2 = np.copy(self.b2)\n",
    "                    l = [W1, b1, W2, b2]\n",
    "                    l[i][row,col] += epsilon\n",
    "                    (ha, hs, oa, os_plus_epsilon) = self.fprop(inputs, l[0], l[1], l[2], l[3])\n",
    "                    l[i][row,col] -= 2* epsilon\n",
    "                    (ha, hs, oa, os_minus_epsilon) = self.fprop(inputs, l[0], l[1], l[2], l[3])\n",
    "                    diff = 0\n",
    "                    if(type(labels)==numpy.int64):\n",
    "                        diff += (self.loss_one_example(os_plus_epsilon, labels,0) - self.loss_one_example(os_minus_epsilon, labels,0)) / (2* epsilon)\n",
    "                    else:\n",
    "                        for k,elem in enumerate(labels):\n",
    "                            diff += (self.loss_one_example(os_plus_epsilon, elem, k) - self.loss_one_example(os_minus_epsilon, elem, k)) / (2* epsilon)\n",
    "                    temp_mat_grad[row,col] = diff/ inputs.shape[1]\n",
    "            grads_finite.append(temp_mat_grad)\n",
    "        return grads_finite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
