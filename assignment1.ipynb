{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.loadtxt(open(\"mnist_train.csv\", \"rb\"), delimiter=\",\", skiprows=1)\n",
    "\n",
    "train_labels = x[:50000, 0]\n",
    "train_data = x[:50000, 1:]\n",
    "\n",
    "valid_labels = x[50000:57000, 0]\n",
    "valid_data = x[50000:57000, 1:]\n",
    "\n",
    "test_labels = x[57000:,0]\n",
    "test_data = x[57000:,1:]\n",
    "\n",
    "unseen_data = np.loadtxt(open(\"mnist_test.csv\", \"rb\"), delimiter=\",\", skiprows=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    \n",
    "    return x*np.int64(x>0)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Th\n",
    "def softmax(x):\n",
    "    #print('avant: ', x)\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    #e_x = np.exp(x - np.max(x))\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "\n",
    "\n",
    "def non_linearity(x, nonlin='relu'):\n",
    "    if nonlin == 'relu':\n",
    "        return x*np.int64(x>0)\n",
    "    elif nonlin == 'sigmoid':\n",
    "        return 1 / (1 + np.exp(-1*x))\n",
    "    elif nonlin == 'softmax':\n",
    "        exps = np.exp(x - np.max(x))\n",
    "        return exps / exps.sum(axis=0)\n",
    "        \n",
    "#//////////////////////////////////////////////////////////////////\n",
    "def rand_index(upper_bound):\n",
    "    return np.random.randint(0, upper_bound)\n",
    "\n",
    "\n",
    "#////////////////////////////////////////////////////////////////////\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "def cross_entropy(predictions, targets):\n",
    "    target = np.eye(10, dtype=int)[int(targets)]\n",
    "    ce = -np.sum(target*np.log(predictions))\n",
    "    return ce\n",
    "\n",
    "\n",
    "# where y is true (1 or 0) and i is the class\n",
    "def loss_one_example(os, y, i):\n",
    "    \n",
    "    return -np.log(os[y,i])\n",
    "\n",
    "def cross_entr_loss(prediction, true_label):\n",
    "    # Calculate cross entropy for each output vector element , then sum up\n",
    "    loss = 0 \n",
    "    y = np.eye(10)[int(true_label)]\n",
    "\n",
    "    for k,elem in enumerate(y):\n",
    "            loss += loss_one_example(prediction[y], elem ,k)\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "def CrossEntropy(yHat, y):\n",
    "    if yHat == 1:\n",
    "        return -np.log(y)\n",
    "    else:\n",
    "        return -np.log(1 - y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three different ways to initialize parameters\n",
    "\n",
    "def initialize_weights( in_dim, out_dim, technique='glorot', sigma=0.002, mu=0):\n",
    "        # Init with zeros for now\n",
    "        if technique =='zeros':\n",
    "            return np.zeros((in_dim,out_dim))\n",
    "        elif technique == 'normal':\n",
    "            return sigma * np.random.randn(in_dim, out_dim) + mu\n",
    "        elif technique == 'glorot':\n",
    "            return np.random.randn(in_dim, out_dim) * np.sqrt(1/out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets of weights are: w_1 and b1 for layer 1, up to layer 3\n",
    "# pre actiavation neuron outputs are referred to as h1_a for layer 1\n",
    "# post activation neuron output are h1_o for layer 1\n",
    "# Note that I note h3 instead of a more common ho for the output layer\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    \n",
    "    def __init__(self, hidden_dims=(200,600),n_hidden=2, step_size = 0.0001, train_data=train_data,\n",
    "                 train_labels=train_labels, valid_data=valid_data, valid_labels=valid_labels, classes=10):\n",
    "        \n",
    "        \n",
    "        self.train_data = train_data\n",
    "        self.train_labels = train_labels\n",
    "        self.valid_data = valid_data\n",
    "        self.valid_labels = valid_labels\n",
    "        \n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.input_dim = train_data.shape[1]\n",
    "        self.classes = classes\n",
    "        \n",
    "        self.w_1 = initialize_weights(hidden_dims[0], self.input_dim)\n",
    "        self.w_2 = initialize_weights(hidden_dims[0],hidden_dims[1])        \n",
    "        self.w_3 = initialize_weights(classes, hidden_dims[1])\n",
    "        \n",
    "        self.b1 = np.zeros(hidden_dims[0])\n",
    "        self.b2 = np.zeros(hidden_dims[1])\n",
    "        self.b3 = np.zeros(classes)\n",
    "        \n",
    " \n",
    "            \n",
    "    def forward_prop(self, x, non_lin='relu'):\n",
    "        # Simply passes forward on input vector\n",
    "        # return output pre and post activation at each layer\n",
    "        h1_a = np.matmul(self.w_1, x) + self.b1 \n",
    "        #print('h1_a: ', h1_a.shape)\n",
    "\n",
    "        h1_o = non_linearity(h1_a, non_lin)\n",
    "        #print('h1_0: ', h1_o.shape)\n",
    "        h2_a = np.matmul(np.transpose(self.w_2) , h1_o) + self.b2 \n",
    "        #print('h2_a: ', h2_a.shape)\n",
    "        h2_o = non_linearity(h2_a, non_lin)\n",
    "        #print('h2_0: ', h2_o.shape)\n",
    "        h3_a = np.matmul(self.w_3 , h2_o) + self.b3 \n",
    "        h3_o = softmax(h3_a)\n",
    "        \n",
    "        return (h1_a, h1_o, h2_a, h2_o, h3_a, h3_o)  \n",
    "    \n",
    "    def backward_prop(self, h1_a, h1_o, h2_a, h2_o, h3_a, h3_o, inputs, labels):\n",
    "        \n",
    "        # Find gradient of loss with respect to each parameters\n",
    "        \n",
    "        stuff = np.eye(self.classes)[int(labels)].reshape(self.classes,)\n",
    "        grad_h3a = h3_o - stuff\n",
    "        grad_b3 = grad_h3a\n",
    "        grad_W3 = np.matmul(grad_h3a, np.transpose(h3_o))\n",
    "        \n",
    "        grad_h2o = np.matmul(np.transpose(self.w_3), grad_h3a)\n",
    "        grad_h2a = grad_h2o * np.int64(h2_a>0)\n",
    "        grad_b2 = grad_h2a\n",
    "        grad_W2 = np.matmul(grad_h2a, np.transpose(h1_o))\n",
    "        \n",
    "        grad_h1o = np.matmul(np.transpose(self.w_2), grad_h2a)\n",
    "        grad_h1a = grad_h1o * np.int64(h1_a > 0)\n",
    "        grad_b1 = grad_h1a\n",
    "        grad_W1 = np.matmul(grad_h1a.reshape(500,1), np.transpose(inputs.reshape(784,1)))\n",
    "                \n",
    "        return (grad_W1, grad_b1, grad_W2, grad_b2, grad_W3, grad_b3)\n",
    "    \n",
    "    # A function that trains a whole minibatch and average the gradients, to be sent for parameter update\n",
    "    def train_minibatch(self, batch_size):\n",
    "        # fprop and bprop for batch_size inputs, each time summing grads\n",
    "        \n",
    "        sum_grad_W1 = np.zeros((self.w_1.shape[0], self.w_1.shape[1]))\n",
    "        sum_grad_W2 = np.zeros((self.w_2.shape[0], self.w_2.shape[1]))\n",
    "        sum_grad_W3 = np.zeros((self.w_3.shape[0], self.w_3.shape[1]))\n",
    "        sum_grad_b1 = np.zeros(self.b1.shape[0])\n",
    "        sum_grad_b2 = np.zeros(self.b2.shape[0])\n",
    "        sum_grad_b3 = np.zeros(self.b3.shape[0])\n",
    "\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            z = rand_index(self.train_data.shape[0])\n",
    "            (h1_a, h1_o, h2_a, h2_o, h3_a, h3_o)  = self.forward_prop(train_data[z,:])\n",
    "            (grad_W1, grad_b1, grad_W2, grad_b2, grad_W3, grad_b3) = self.backward_prop(h1_a, h1_o, h2_a, h2_o, h3_a, h3_o, train_data[z,:] , train_labels[z] )\n",
    "            sum_grad_W1 = np.add(sum_grad_W1, grad_W1)\n",
    "            sum_grad_b1 = np.add(sum_grad_b1, grad_b1)\n",
    "            sum_grad_W2 = np.add(sum_grad_W2, grad_W2)\n",
    "            sum_grad_b2 = np.add(sum_grad_b2, grad_b2)\n",
    "            sum_grad_W3 = np.add(sum_grad_W3, grad_W3)\n",
    "            sum_grad_b3 = np.add(sum_grad_b3, grad_b3)\n",
    "        # return average of each gradients over the batch\n",
    "        return (sum_grad_W1/batch_size, sum_grad_b1/batch_size, sum_grad_W2/batch_size, sum_grad_b2/batch_size, \n",
    "                sum_grad_W3/batch_size, sum_grad_b3/batch_size)\n",
    "            \n",
    "    \n",
    "    \n",
    "    #Now that we have gradients, build a function to update all values using those gradient\n",
    "    def update_params(self, grad_W1, grad_b1, grad_W2, grad_b2, grad_W3, grad_b3, step_size):\n",
    "        self.w_1 = self.w_1 - (step_size * grad_W1 )\n",
    "        self.b1 = self.b1 - (step_size * grad_b1)\n",
    "        self.w_2 = self.w_2 - (step_size * grad_W2)\n",
    "        self.b2 = self.b2 - (step_size * grad_b2)\n",
    "        self.w_3 = self.w_3 - (step_size * grad_W3)\n",
    "        self.b3 = self.b3 - (step_size * grad_b3)\n",
    "        \n",
    "    # Full training, for n epochs\n",
    "    def train(self, batch_size, epochs, step_size):\n",
    "        for j in range(epochs):\n",
    "            sum_loss = 0\n",
    "            (grad_w1, grad_b1, grad_w2, grad_b2, grad_w3, grad_b3 ) = self.train_minibatch(batch_size)\n",
    "            self.update_params(grad_w1, grad_b1, grad_w2, grad_b2, grad_w3, grad_b3, step_size)\n",
    "            # after updating, calculate and output average loss\n",
    "            sum_loss = self.average_train_loss()\n",
    "            print('Average loss on training set after ',j+1 ,' epochs: ', sum_loss)\n",
    "            #print('Misclassification: ', self.test_misclass(train_data, train_labels), '%')\n",
    "            #print('-----------------------') \n",
    "\n",
    "        #then calculate loss on validation set\n",
    "        valid_loss = self.average_valid_loss()\n",
    "        print('Loss on validation set after ', (epochs+1),' epochs of training (with step size = ', step_size,'): ', valid_loss)\n",
    "        \n",
    "        print('Misclassification: ', self.test_misclass(valid_data, valid_labels), '%')\n",
    "        print('-----------------------') \n",
    "        \n",
    "        \n",
    "    # prediction function for 1 input, with current mlp parameters values\n",
    "    def predict(self, inputs):\n",
    "        x = np.matmul(self.w_1, inputs) + self.b1\n",
    "        x = relu(x)\n",
    "        x = np.matmul(self.w_2, x) + self.b2\n",
    "        x = relu(x)\n",
    "        x = np.matmul(self.w_3, x) + self.b3\n",
    "        output = softmax(x)\n",
    "        \n",
    "        return argmax(output)\n",
    "    \n",
    "    # calculate average loss over whole train set\n",
    "    def average_train_loss(self):\n",
    "        loss = 0\n",
    "        # summing loss\n",
    "        for k in range(self.train_data.shape[0]):\n",
    "            randindex = rand_index(self.train_data.shape[0])\n",
    "            (h1_a, h1_o, h2_a, h2_o, h3_a, h3_o) = self.forward_prop(train_data[randindex,...])    \n",
    "            one_loss = np.sum(cross_entropy(h3_o, train_labels[randindex]))\n",
    "            loss += one_loss\n",
    "            \n",
    "        return loss/self.train_data.shape[0]\n",
    "        \n",
    "    def average_valid_loss(self):\n",
    "        loss = 0\n",
    "        # summing loss\n",
    "        for k in range(self.valid_data.shape[0]):\n",
    "            randindex = rand_index(self.valid_data.shape[0])\n",
    "            (h1_a, h1_o, h2_a, h2_o, h3_a, h3_o) = self.forward_prop(valid_data[randindex,...])    \n",
    "            one_loss = np.sum(cross_entropy(h3_o, valid_labels[randindex]))\n",
    "            loss += one_loss\n",
    "        return (loss/self.train_data.shape[0])\n",
    "    \n",
    "    def test_misclass(self, data, labels):\n",
    "        misclass = 0\n",
    "        \n",
    "        for k in range(data.shape[0]):\n",
    "            (h1_a, h1_o, h2_a, h2_o, h3_a, h3_o) = self.forward_prop(data[k,:])\n",
    "            pred = np.argmax(h3_o)\n",
    "            if(pred != int(labels[k])):\n",
    "                misclass += 1\n",
    "        \n",
    "        return ((misclass/data.shape[0])*100 )\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.MLP object at 0x0000016A006029B0>\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP()\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output vector:  [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n"
     ]
    }
   ],
   "source": [
    "# pred output --> (h1_a, h1_o, h2_a, h2_o, h3_a, h3_o) \n",
    "pred = mlp.forward_prop(train_data[4,...])\n",
    "output_vect = pred[5]\n",
    "print('output vector: ', output_vect)\n",
    "\n",
    "#loss_sum = cross_entropy(output_vect, int(train_labels[4]) )\n",
    "#print(np.sum(loss_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#self, h1_a, h1_o, h2_a, h2_o, h3_a, h3_o, inputs, labels\n",
    "#(grad_W1, grad_b1, grad_W2, grad_b2, grad_W3, grad_b3) = mlp.backward_prop(pred[0], pred[1], pred[2], pred[3], pred[4], pred[5], train_data[1,...], np.argmax(train_labels[4]))\n",
    "\n",
    "#print(grad_W1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on training set after  1  epochs:  2.318167632733662\n",
      "Average loss on training set after  2  epochs:  2.317117969603752\n",
      "Average loss on training set after  3  epochs:  2.3117237044459706\n",
      "Average loss on training set after  4  epochs:  2.3128296617430633\n",
      "Average loss on training set after  5  epochs:  2.3115271142787863\n",
      "Average loss on training set after  6  epochs:  2.3114278464696123\n",
      "Average loss on training set after  7  epochs:  2.310222790445528\n",
      "Average loss on training set after  8  epochs:  2.309907272840814\n",
      "Average loss on training set after  9  epochs:  2.309838919071448\n",
      "Average loss on training set after  10  epochs:  2.3077291211154622\n",
      "Loss on validation set after  11  epochs of training (with step size =  1 ):  0.32306161328940874\n",
      "Misclassification:  90.3 %\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "#def train(self, batch_size, epochs, step_size):\n",
    "mlp2 = MLP()\n",
    "mlp2.train(128, 10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (600,) and (200,) not aligned: 600 (dim 0) != 200 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-228-34e8fb5add1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#def train(self, batch_size, epochs, step_size):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmlp3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmlp3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-226-741d2b9364f4>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, batch_size, epochs, step_size)\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[0msum_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;33m(\u001b[0m\u001b[0mgrad_w1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_b1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_w2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_b2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_w3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_b3\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_minibatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_w1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_b1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_w2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_b2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_w3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_b3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[1;31m# after updating, calculate and output average loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-226-741d2b9364f4>\u001b[0m in \u001b[0;36mtrain_minibatch\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrand_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[0mh1_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh1_o\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh2_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh2_o\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh3_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh3_o\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_prop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m             \u001b[1;33m(\u001b[0m\u001b[0mgrad_W1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_b1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_W2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_b2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_W3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_b3\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward_prop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh1_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh1_o\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh2_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh2_o\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh3_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh3_o\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m             \u001b[0msum_grad_W1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum_grad_W1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_W1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[0msum_grad_b1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum_grad_b1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_b1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-226-741d2b9364f4>\u001b[0m in \u001b[0;36mbackward_prop\u001b[1;34m(self, h1_a, h1_o, h2_a, h2_o, h3_a, h3_o, inputs, labels)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mgrad_h2a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrad_h2o\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh2_a\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mgrad_b2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrad_h2a\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[0mgrad_W2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_h2a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh1_o\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mgrad_h1o\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_h2a\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (600,) and (200,) not aligned: 600 (dim 0) != 200 (dim 0)"
     ]
    }
   ],
   "source": [
    "#def train(self, batch_size, epochs, step_size):\n",
    "mlp3 = MLP()\n",
    "mlp3.train(128, 2, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassification rate on test set:  90.4 %\n"
     ]
    }
   ],
   "source": [
    "# calculate 1-0 loss on test seen\n",
    "mlp2.test_misclass(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was taken from my Homework 3 of Ioannis' Machine Learning class in Automn '18'\n",
    "def finite_diff(self, inputs, labels):\n",
    "        #for each parameters we vary by epsilon one of its value and compute the finite gradient\n",
    "        #(f(x-eps) - f(x+eps))/eps\n",
    "        epsilon = 10**(-4)\n",
    "        grads_finite = []\n",
    "        for i,mat in enumerate([self.W1, self.b1, self.W2, self.b2]):\n",
    "            temp_mat_grad = np.zeros((len(mat), len(mat[0])))\n",
    "            for row in range(len(mat)):\n",
    "                for col in range(len(mat[0])):\n",
    "                    W1 = np.copy(self.W1)\n",
    "                    b1 = np.copy(self.b1)\n",
    "                    W2 = np.copy(self.W2)\n",
    "                    b2 = np.copy(self.b2)\n",
    "                    l = [W1, b1, W2, b2]\n",
    "                    l[i][row,col] += epsilon\n",
    "                    (ha, hs, oa, os_plus_epsilon) = self.fprop(inputs, l[0], l[1], l[2], l[3])\n",
    "                    l[i][row,col] -= 2* epsilon\n",
    "                    (ha, hs, oa, os_minus_epsilon) = self.fprop(inputs, l[0], l[1], l[2], l[3])\n",
    "                    diff = 0\n",
    "                    if(type(labels)==numpy.int64):\n",
    "                        diff += (self.loss_one_example(os_plus_epsilon, labels,0) - self.loss_one_example(os_minus_epsilon, labels,0)) / (2* epsilon)\n",
    "                    else:\n",
    "                        for k,elem in enumerate(labels):\n",
    "                            diff += (self.loss_one_example(os_plus_epsilon, elem, k) - self.loss_one_example(os_minus_epsilon, elem, k)) / (2* epsilon)\n",
    "                    temp_mat_grad[row,col] = diff/ inputs.shape[1]\n",
    "            grads_finite.append(temp_mat_grad)\n",
    "        return grads_finite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
